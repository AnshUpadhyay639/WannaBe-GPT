{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkx3gBBGoZox2KP1ZGcAyp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshUpadhyay639/WannaBe-GPT/blob/main/Wannabe_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "**1)** The official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "\n",
        "**2)** Huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n"
      ],
      "metadata": {
        "id": "DiNUI0lxDk93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hah8qO8UDE8Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import glob\n",
        "import struct\n",
        "import inspect\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch._inductor.config as config\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "FdC7MLgKD5eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch nn.Module implementation for GeLU Activation:\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"This is the GeLU version which is the exact one used by OpenAI in GPT-2\"\"\"\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
      ],
      "metadata": {
        "id": "lo7t6Cm8D8G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass # This is used to automatically generate special methods for a class, such as __init__(), __repr__(), __eq__(), and others without us defining custom implementations for it.\n",
        "class GPTConfig: # Configurations || Hyperparameters.\n",
        "    block_size: int = 1024 # Context Length : Maximum number of tokens the Transformer can process in one forward pass.[Same as Batch Size].\n",
        "    vocab_size: int = 50257 # No. of Unique Characters the Input is composed of. [Change it to 2^x = 50304 for even GPU cores utilization].\n",
        "    n_layer: int = 12 # No. of Layers || Blocks in a Transformer.\n",
        "    n_head: int = 12 # No. of Attn Heads.\n",
        "    n_embd: int = 768 # No. of Features || Columns : Used to represent each token.[Each token is represented as a single Horizontal Vector of 768].\n"
      ],
      "metadata": {
        "id": "rw8kKCzTFq1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a global to toggle flash-attention\n",
        "FLASH = 0 # For Toggling Faster Scaled Dot Product Attention by Kernel Fusioning the Self Attn Snippets."
      ],
      "metadata": {
        "id": "CzUo8CuAEOZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-Attention Implementation:\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 , \"Error\" # If condition is True, the program continues else throws an error (String) & stop.\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # If 1 then scale wts for all Layers|| Blocks due to Residual Connections.\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        if FLASH:\n",
        "            # flashattention\n",
        "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # Convert the Else block lines into a single line (Kernel Fusion).\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            # this materializes the large (T,T) matrix for all the queries and keys\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "2rk9tELGEntb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of a Multi-Layered Perceptron (MLP) || (FCC) Fully Connected Layer:\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = NewGELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qPjxQYWtFNro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of a Transformer Block or Layer:\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # First Self Attn happens followed by FCC||MLP.\n",
        "        x = x + self.mlp(self.ln_2(x)) # Residual Connections.\n",
        "        return x"
      ],
      "metadata": {
        "id": "erQHPddoFYnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main GPT Model starts from here:\n"
      ],
      "metadata": {
        "id": "89926SFnGW5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config # Stores all the Hyperparameters for a custom Model.\n",
        "\n",
        "        # Create a Transformer Module which stores info about Model's inner states.\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd), # Stores Weights of Token Embeddings.\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd), # Stores Weights of Positional Embeddings.\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Stores the List of Layers in the Transformers.\n",
        "            ln_f = nn.LayerNorm(config.n_embd), # Final LayerNorm.\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier Layer [bias = False due to LayerNorm].\n",
        "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights.[FLAG]\n",
        "        self.transformer.wte.weight = self.lm_head.weight # WEIGHT TYING: https://paperswithcode.com/method/weight-tying [I/P emb == O/P emb][Saves Memory][Efficiency++ && Time--][Makes Similar tokens having Similar Wts and Probs].\n",
        "\n",
        "        # init all weights, use a torch rng object to be very careful\n",
        "        self.init_rng = torch.Generator()\n",
        "        self.init_rng.manual_seed(42)\n",
        "        self.apply(self._init_weights) # Iterates and Applies the specific function on all the above modules.[Applying Wt Initialization on all sub modules above].\n",
        "\n",
        "    def _init_weights(self, module): # Takes an nn.Module as input and initialize its Weights.\n",
        "        if isinstance(module, nn.Linear): # If a nn.Linear() Layer then;\n",
        "            # Using Residual Connections increases Variance && std() so we have to counter it:\n",
        "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2 * self.config.n_layer) # Divide std() by sqrt(Rn) [No. of Residual Connections(Rn) = ResidualConnec in each block * No. of Blocks||Layers].\n",
        "            # We want to skip initializing lm_head, which shares parameters with wte.\n",
        "            # and wte was already initialized down below during the Embedding init.\n",
        "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng) # Normal (Gaussian) distribution.\n",
        "            if module.bias is not None: # If Linear layer has bias ; Make it 0.\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding): # If nn.Embedding() then;\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng) # Normal (Gaussian) distribution.\n",
        "\n",
        "    def forward(self, idx, targets=None, return_logits=True):\n",
        "        device = idx.device # Change Inputs = idx to target device.\n",
        "        b, t = idx.size() # Inputs are always in shape[Batch, Tokens] with Columns = Num_features = Context length = Block Size.\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd).\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd).\n",
        "        x = tok_emb + pos_emb # Broadcasting for missing dim = B in pos_emb.\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss.\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1) # Flatten the logits to [B*T, Vocab] for input to CrossEntropy() ; Targets become [B.T].\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position.\n",
        "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] to preserve the time dim [These Logits predicts the (B,T+1)th Token].\n",
        "            loss = None\n",
        "\n",
        "        # there are performance reasons why not returning logits is prudent, if not needed.\n",
        "        if not return_logits:\n",
        "            logits = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod # Makes the below function a Constructor of the Class [Notice how its takes input as \"cls\" to refer to the Class intead of \"self\"].\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Weight decay is important in an optimizer because it helps regularize the model by preventing overfitting and improving generalization. It does this by adding a penalty to large weights, encouraging smaller weight values that make the model more stable.\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage): # Set the Weight Decay for specific parameters.\n",
        "        # Fetch all the Model Parameters.\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # Filter out those that do not require grad.\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad} # Filter out 'requires_grad = False'.\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        # Split out the params into single dim (like biases) && multi dim.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0} # Dont Wt. decay the Biases and LayerNorms.\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params) # '.numel()' - returns the number of elements in a Tensor.\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available:\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        print0(f\"using fused AdamW: {use_fused}\") # Fuse : Is just the Kernel Fusion intead of an iterative param update for AdamW Optimizer.\n",
        "        if zero_stage == 1:\n",
        "            print(\"using ZeroRedundancyOptimizer\")\n",
        "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
        "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
        "            optimizer.add_param_group(optim_groups[1])\n",
        "        else:\n",
        "            print0(\"using regular AdamW\")\n",
        "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "FZCYRkOlHxVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _peek_data_shard(filename): # Shard: is a single numpy file or array like torch.tensors() of the input dataset which is divided into multiple shards.\n",
        "    # only reads the header, returns header data\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "    if header[0] != 20240520:\n",
        "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
        "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
        "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
        "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
        "        exit(1)\n",
        "    assert header[1] == 1, \"unsupported version\"\n",
        "    ntok = header[2] # number of tokens (claimed)\n",
        "    return ntok # for now just return the number of tokens"
      ],
      "metadata": {
        "id": "aaryP4fceOr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data_shard(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
        "        assert header[1] == 1, \"unsupported version\"\n",
        "        ntok = header[2] # number of tokens (claimed)\n",
        "        # the rest of it are tokens, stored as uint16\n",
        "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ybRquvifeSsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Self Made DataLoader:\n",
        "class DistributedDataLoader:\n",
        "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
        "        # Setting our DataLoader according to DDP [Distributed Data Parallel] for using multiple GPUs at a time.\n",
        "        self.process_rank = process_rank # Specifies the rank of a process .\n",
        "        self.num_processes = num_processes # [A single process handles = Whole Code/Num of GPUs][No. of Process = No. of GPUs].\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # glob files that match the pattern\n",
        "        self.files = sorted(glob.glob(filename_pattern))\n",
        "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
        "\n",
        "        # load and validate all data shards, count number of tokens in total\n",
        "        ntok_total = 0\n",
        "        for fname in self.files:\n",
        "            shard_ntok = _peek_data_shard(fname)\n",
        "            assert shard_ntok >= num_processes * B * T + 1\n",
        "            ntok_total += shard_ntok\n",
        "        self.ntok_total = ntok_total\n",
        "        print(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
        "\n",
        "        # kick things off\n",
        "        self.current_shard = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
        "        # then don't do the work to reload it, just reset the pointer\n",
        "        if self.current_shard != 0:\n",
        "            self.current_shard = 0\n",
        "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "        self.current_position = self.process_rank * self.B * self.T\n",
        "\n",
        "    def advance(self): # advance to next data shard\n",
        "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
        "        self.current_position = self.process_rank * self.B * self.T # If process rank = 0 (1st Process) then the start idx will also be 0 and so-on.\n",
        "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "\n",
        "    def next_batch(self):\n",
        "        B = self.B\n",
        "        T = self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1] # At idx = B.T+1 is the O/P prediction for the Last B.T Token.\n",
        "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
        "        x = (buf[:-1]).view(B, T) # Inputs. [ Everything excpet the (B.T + 1)th or Last Token].\n",
        "        y = (buf[1:]).view(B, T) # Targets [Skip the 1st with an offset of 1].\n",
        "        # advance the start pointer in current shard\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds advance the shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.advance()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "C2_SfPZJInIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities for saving params/grads/activations to **.bin** files:\n"
      ],
      "metadata": {
        "id": "8CfqWeQh0ALb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_fp32(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.float32)\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "Zv1eX04l0HZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_bf16(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
        "    # numpy doesn't have bf16 datatype so we have to trick it\n",
        "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "Fz_I9bOI0HV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tensors(model_tensors, L, file, dtype):\n",
        "    # writes the GPT Model's weights to a binary file\n",
        "    assert dtype in {\"float32\", \"bfloat16\"}\n",
        "    write_fun = write_fp32 if dtype == \"float32\" else write_bf16\n",
        "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
        "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
        "    for i in range(L): # (L, 3C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
        "    for i in range(L): # (L, 3C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
        "    for i in range(L): # (L, C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
        "    for i in range(L): # (L, 4C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
        "    for i in range(L): # (L, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
        "    for i in range(L): # (L, C, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
        "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
        "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )"
      ],
      "metadata": {
        "id": "BJakQhxr0HTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def pad_vocab(tensor, multiple=128, value=0): # Padding our Vocab to make the size closer to 2^x for efficient GPU Core Utilization.\n",
        "    \"\"\"\n",
        "    The dimension of the vocab size in GPT-2 is 50,257\n",
        "    which is unfortunately a very ugly number for a lot of\n",
        "    matrix operations on the GPU. So we pad it to the nearest\n",
        "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
        "    export the weights into C land. This is a NOOP algorithmically\n",
        "    and is only done to make the tensor operations more efficient.\n",
        "    \"\"\"\n",
        "    assert tensor.ndim == 2\n",
        "    V, C = tensor.shape\n",
        "    assert V == 50257, \"just being defensive here\"\n",
        "    # calculate padded vocab size by rounding up to nearest multiple\n",
        "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
        "    # pad the tensor\n",
        "    pad_rows = Vp - V\n",
        "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
        "    assert padded.shape == (Vp, C)\n",
        "    return padded"
      ],
      "metadata": {
        "id": "dlYGvvKT0HQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_model(model, filename, dtype):\n",
        "    # everything we need to instantiate the model\n",
        "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
        "    assert dtype in {\"float32\", \"bfloat16\"} # float16 todo maybe later\n",
        "    version = {\n",
        "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
        "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
        "    }[dtype]\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240326 # magic\n",
        "    header[1] = version # checkpoint version\n",
        "    header[2] = model.config.block_size\n",
        "    header[3] = model.config.vocab_size\n",
        "    header[4] = model.config.n_layer\n",
        "    header[5] = model.config.n_head\n",
        "    header[6] = model.config.n_embd\n",
        "    # 2) the parameters follow the header\n",
        "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
        "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
        "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
        "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
        "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
        "    # now write to file\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes()) # header\n",
        "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "BYhqagVu0HOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_state(model, x, y, logits, loss, filename):\n",
        "    # the state is used for debugging.\n",
        "    # it contains information about the input, logits, loss, and the parameter gradients\n",
        "    # this can be used for checking the computation correctness in C\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240327 # magic\n",
        "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
        "    header[2] = x.size(0) # batch size of the batch, B\n",
        "    header[3] = x.size(1) # temporal extent of the batch, T\n",
        "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab grads here as well, to mirror write_model\n",
        "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
        "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
        "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
        "    with open(filename, \"wb\") as file:\n",
        "        # header\n",
        "        file.write(header.numpy().tobytes())\n",
        "        # input x\n",
        "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # targets y\n",
        "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # logits (result of the model forward pass)\n",
        "        write_fp32(logits.cpu(), file)\n",
        "        # loss (single float, result of the cross entropy loss)\n",
        "        write_fp32(loss.cpu(), file)\n",
        "        # gradients\n",
        "        write_tensors(grads, model.config.n_layer, file, \"float32\")\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "gNKwVOxQ0HLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tokenizer(enc, filename):\n",
        "    n = enc.max_token_value + 1\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240328 # magic\n",
        "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
        "    header[2] = n # number of tokens\n",
        "    header[3] = enc.eot_token # EOT token\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes())\n",
        "        for i in range(n):\n",
        "            b = enc.decode_bytes([i])\n",
        "            length = len(b)\n",
        "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
        "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
        "            file.write(b)  # Write the actual bytes\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "Uu56Vt-10HIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print0(*args, **kwargs):\n",
        "    # modified print that only prints from the master process\n",
        "    # if this is not a distributed run, it's just a print\n",
        "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
        "        print(*args, **kwargs)"
      ],
      "metadata": {
        "id": "XelCIJDl0HFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!python data/tinyshakespeare.py --model=gpt-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZTIa0gwwyB-",
        "outputId": "12c5a870-1b84-46f0-8204-62f23ebd8d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "/content/data/tinyshakespeare/tiny_shakespeare.txt already exists, skipping download...\n",
            "writing 32,768 tokens to /content/data/tinyshakespeare/tiny_shakespeare_val.bin (66,560 bytes) in the gpt-2 format\n",
            "writing 305,260 tokens to /content/data/tinyshakespeare/tiny_shakespeare_train.bin (611,544 bytes) in the gpt-2 format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** When a script is run directly, __name__ is set to \"__main__\".\n",
        "When a script is imported as a module, __name__ is set to the module's name (i.e., the filename without .py)."
      ],
      "metadata": {
        "id": "yBQdYxQ5wZYX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-KSbZuIxeD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    import argparse\n",
        "    import tiktoken\n",
        "    print0(f\"Running pytorch {torch.version.__version__}\")\n",
        "\n",
        "    # default settings will overfit a tiny batch of data\n",
        "    # and save model weights and debug state to disk on the first iteration\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # file system input / output\n",
        "    parser.add_argument(\"--input_bin\", type=str, default=\"data/tinyshakespeare/tiny_shakespeare_val.bin\", help=\"input .bin to train on\")\n",
        "    parser.add_argument(\"--input_val_bin\", type=str, default=\"\", help=\"input .bin to eval validation loss on\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"output directory to which to write logs and checkpoints\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"gpt2\", help=\"gpt2|gpt2-medium|gpt2-large|gpt2-xl|d12|d24|d36|d48\")\n",
        "    # token layout for each step of the optimization\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size, in units of #batch dimensions\")\n",
        "    parser.add_argument(\"--sequence_length\", type=int, default=64, help=\"sequence length\")\n",
        "    parser.add_argument(\"--total_batch_size\", type=int, default=256, help=\"total desired batch size, in units of #tokens\")\n",
        "    # workload (number of steps)\n",
        "    parser.add_argument(\"--num_iterations\", type=int, default=10, help=\"number of iterations to run\")\n",
        "    parser.add_argument(\"--inference_only\", type=int, default=0, help=\"only run inference\")\n",
        "    # optimization\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--learning_rate_decay_frac\", type=float, default=1.0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight decay\")\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"maximum gradient magnitude\")\n",
        "    # evaluation\n",
        "    parser.add_argument(\"--val_loss_every\", type=int, default=0, help=\"every how mant steps to evaluate val loss?\")\n",
        "    parser.add_argument(\"--val_max_steps\", type=int, default=20, help=\"how many batches of val to average?\")\n",
        "    parser.add_argument(\"--sample_every\", type=int, default=0, help=\"how often to sample from the model?\")\n",
        "    # debugging\n",
        "    parser.add_argument(\"--overfit_single_batch\", type=int, default=1, help=\"overfit just one batch of data\")\n",
        "    # numerics\n",
        "    parser.add_argument(\"--tensorcores\", type=int, default=0, help=\"use tensorcores\")\n",
        "    # memory management\n",
        "    parser.add_argument(\"--device\", type=str, default=\"\", help=\"by default we autodetect, or set it here\")\n",
        "    parser.add_argument(\"--compile\", type=int, default=0, help=\"torch.compile the model\")\n",
        "    parser.add_argument(\"--flash\", type=int, default=0, help=\"use flash attention\")\n",
        "    parser.add_argument(\"--dtype\", type=str, default=\"float32\", help=\"float32|float16|bfloat16\")\n",
        "    parser.add_argument(\"--zero_stage\", type=int, default=0, help=\"zero redundancy optimizer stage (0/1/2/3)\")\n",
        "    # python -> C bridge\n",
        "    parser.add_argument(\"--write_tensors\", type=int, default=1, help=\"write tensors to disk\")\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # args error checking and convenience variables\n",
        "    B, T = args.batch_size, args.sequence_length\n",
        "    assert 1 <= T <= 1024\n",
        "    assert args.dtype in {\"float32\", \"float16\", \"bfloat16\"}\n",
        "    assert args.model in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"d12\", \"d24\", \"d36\", \"d48\"}\n",
        "\n",
        "    # set up DDP (distributed data parallel). torchrun sets this env variable\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "    if ddp:\n",
        "        # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "        assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "        init_process_group(backend='nccl')\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device)\n",
        "        master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "        seed_offset = 0 # each process gets the exact same seed\n",
        "        zero_stage = args.zero_stage\n",
        "    else:\n",
        "        ddp_rank = 0\n",
        "        ddp_local_rank = 0\n",
        "        zero_stage = 0\n",
        "        ddp_world_size = 1\n",
        "        master_process = True\n",
        "        seed_offset = 0\n",
        "        # select the device\n",
        "        if args.device:\n",
        "            # provided explicitly by the user\n",
        "            device = args.device\n",
        "        else:\n",
        "            # attempt to autodetect the device\n",
        "            device = \"cpu\"\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "                device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "    device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "\n",
        "    # calculate gradient accumulation from the desired total batch size and the current run configuration\n",
        "    tokens_per_fwdbwd = B * T * ddp_world_size\n",
        "    assert args.total_batch_size % tokens_per_fwdbwd == 0\n",
        "    grad_accum_steps = args.total_batch_size // tokens_per_fwdbwd\n",
        "    print0(f\"total desired batch size: {args.total_batch_size}\")\n",
        "    print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "    # set up a context manager following the desired dtype and device\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[args.dtype]\n",
        "    ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
        "\n",
        "    # rng / reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # set the torch precision mode to use TensorFloat32 (TF32) for matmuls\n",
        "    # docs https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
        "    if args.tensorcores:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # turn on/off flash attention\n",
        "    assert args.flash in {0, 1}\n",
        "    FLASH = args.flash\n",
        "\n",
        "    # init (and write) the tokenizer\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    if master_process and args.write_tensors: # tokenizer is technically not tensors but ok\n",
        "        write_tokenizer(enc, \"gpt2_tokenizer.bin\")\n",
        "\n",
        "    # init the model, either from scratch or from OpenAI pretrained checkpoint\n",
        "    if args.model[0] == \"d\":\n",
        "        # from scratch (random weights)\n",
        "        model_config = {\n",
        "            \"d12\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),\n",
        "            \"d24\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=24, n_head=16, n_embd=1024),\n",
        "            \"d36\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=36, n_head=20, n_embd=1280),\n",
        "            \"d48\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[args.model]\n",
        "        model = GPT(model_config)\n",
        "    else:\n",
        "        # load the GPT-2 model weights\n",
        "        model = GPT.from_pretrained(args.model)\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    if args.compile:\n",
        "        if hasattr(config, \"coordinate_descent_tuning\"):\n",
        "            config.coordinate_descent_tuning = True # suggested by @Chillee\n",
        "        print0(\"compiling the model...\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Our own version of a simple DistributedDataLoader\n",
        "\n",
        "    # load tokens\n",
        "    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n",
        "    val_loader = None\n",
        "    if args.input_val_bin:\n",
        "        val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # PyTorch -> C bridge: save some weights and state for C to load later as reference\n",
        "\n",
        "    # do one forward pass to generate ground truth for our C tests\n",
        "    if master_process and args.write_tensors and (not args.inference_only):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        # save model params, in both float32 and bfloat16\n",
        "        model_to_size = {\"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
        "        model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
        "        model_size_str = model_to_size[args.model] # e.g. \"124M\", or \"d12\"\n",
        "        write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float32\")\n",
        "        write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
        "        # save x, y, logits, loss, and parameter gradients, for debugging C\n",
        "        # always store these in fp32 to have an accurate reference (?)\n",
        "        write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
        "        # reset the train_loader for the optimization below\n",
        "        train_loader.reset()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # main training loop\n",
        "\n",
        "    # here we wrap model into DDP container\n",
        "    if ddp:\n",
        "        model = DDP(model, device_ids=[ddp_local_rank])\n",
        "    raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "    # init the optimizer\n",
        "    optimizer = raw_model.configure_optimizers(weight_decay=args.weight_decay,\n",
        "                                               learning_rate=args.learning_rate, betas=(0.9, 0.95),\n",
        "                                               device_type=device, zero_stage=zero_stage)\n",
        "\n",
        "    # learning rate decay scheduler (cosine with warmup)\n",
        "    def get_lr(it):\n",
        "        min_lr = args.learning_rate * args.learning_rate_decay_frac\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it < args.warmup_iters:\n",
        "            return args.learning_rate * (it+1) / args.warmup_iters\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it > args.num_iterations:\n",
        "            return min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "        return min_lr + coeff * (args.learning_rate - min_lr)\n",
        "\n",
        "    # create the logging directory if it does not exist\n",
        "    logfile = None\n",
        "    if args.output_dir:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "        logfile = os.path.join(args.output_dir, \"main.log\")\n",
        "        # create the log file \"main.log\" inside it, and wipe it clean\n",
        "        with open(logfile, \"w\") as f:\n",
        "            pass\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    timings = []\n",
        "    norm = -1.0   # dummy value to print in inference-only mode\n",
        "    for step in range(args.num_iterations + 1):\n",
        "        t0 = time.time()\n",
        "        last_step = (step == args.num_iterations)\n",
        "\n",
        "        # once in a while evaluate the validation dataset\n",
        "        if (args.val_loss_every > 0 \\\n",
        "            and (step % args.val_loss_every == 0 or last_step)) \\\n",
        "            and (val_loader is not None):\n",
        "            model.eval()\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0.0\n",
        "                for _ in range(args.val_max_steps):\n",
        "                    x, y = val_loader.next_batch()\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    _, loss = model(x, y, return_logits=False)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= args.val_max_steps\n",
        "            # log to console and to file\n",
        "            print0(f\"val loss {val_loss}\")\n",
        "            if master_process and logfile is not None:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(\"s:%d tel:%f\\n\" % (step, val_loss))\n",
        "\n",
        "        # once in a while perform model inference on the master process\n",
        "        if (args.sample_every > 0 \\\n",
        "            and (step % args.sample_every == 0 or last_step)) \\\n",
        "            and master_process:\n",
        "            model.eval()\n",
        "            # before we end, let's also do one round of inference\n",
        "            # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
        "            start_ids = [enc.eot_token]\n",
        "            xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "            max_new_tokens = 32\n",
        "            temperature = 1.0\n",
        "            top_k = 40\n",
        "            yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print0('---------------')\n",
        "            print0(enc.decode(yg[0].tolist()))\n",
        "            print0('---------------')\n",
        "\n",
        "        # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
        "        # but also after the very last iteration. so we loop for step <= num_iterations\n",
        "        # instead of just < num_iterations (one extra due to <=), only to do\n",
        "        # the validation/sampling one last time, and then we break right here as we're done.\n",
        "        if last_step:\n",
        "            break\n",
        "\n",
        "        # --------------- TRAINING SECTION BEGIN -----------------\n",
        "        model.train()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # if we are trying to overfit a single batch, we reset the loader here\n",
        "        if args.overfit_single_batch:\n",
        "            train_loader.reset()\n",
        "        # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
        "        lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # fetch a batch\n",
        "            x, y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            if ddp:\n",
        "                # we want only the last micro-step to sync grads in a DDP model\n",
        "                # the official way to do this is with model.no_sync(), but that is a\n",
        "                # context manager that bloats the code, so we just toggle this variable\n",
        "                model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "            # forward pass\n",
        "            with ctx:\n",
        "                _, loss = model(x, y, return_logits=False)\n",
        "                # we have to scale the loss to account for gradient accumulation,\n",
        "                # because the gradients just add on each successive backward().\n",
        "                # addition of gradients corresponds to a SUM in the objective, but\n",
        "                # instead of a SUM we want MEAN, so we scale the loss here\n",
        "                loss = loss / grad_accum_steps\n",
        "                lossf += loss.detach() # keep track of the mean loss\n",
        "            # backward pass\n",
        "            if not args.inference_only:\n",
        "                loss.backward()\n",
        "        if ddp:\n",
        "            dist.all_reduce(lossf, op=dist.ReduceOp.AVG)\n",
        "        lossf = lossf.item()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # step the optimizer\n",
        "        optimizer.step()\n",
        "        # --------------- TRAINING SECTION END -------------------\n",
        "        # everything that follows now is just diagnostics, prints, logging, etc.\n",
        "\n",
        "        # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
        "        if device == \"mps\":\n",
        "            torch.mps.synchronize()\n",
        "        elif device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        # time and print\n",
        "        t1 = time.time()\n",
        "        # the 0th iteration is often an outlier (much slower) => skip logging it\n",
        "        tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
        "        print0(f\"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
        "        # log to logile\n",
        "        if master_process and logfile is not None:\n",
        "            with open(logfile, \"a\") as f:\n",
        "                f.write(\"s:%d trl:%f\\n\" % (step, lossf))\n",
        "\n",
        "        # keep track of smooth timings, last 20 iterations\n",
        "        if step > 0 and step > args.num_iterations - 20:\n",
        "            timings.append(t1-t0)\n",
        "\n",
        "    # print the average of the last 20 timings, to get something smooth-ish\n",
        "    timings = timings[-20:]\n",
        "    print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
        "    print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # clean up nice\n",
        "    if ddp:\n",
        "        destroy_process_group()"
      ],
      "metadata": {
        "id": "_Yuhd8LO0HDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdd3ffe-a425-48e6-b0bc-4bd695b33994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pytorch 2.5.1+cu124\n",
            "using device: cpu\n",
            "total desired batch size: 256\n",
            "=> calculated gradient accumulation steps: 1\n",
            "wrote gpt2_tokenizer.bin\n",
            "loading weights from pretrained gpt: gpt2\n",
            "DataLoader: total number of tokens: 32,768 across 1 files\n",
            "padded vocab size from 50257 to 50304\n",
            "wrote gpt2_124M.bin\n",
            "padded vocab size from 50257 to 50304\n",
            "wrote gpt2_124M_bf16.bin\n",
            "padded vocab size in reference grads from 50257 to 50304\n",
            "wrote gpt2_124M_debug_state.bin\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: False\n",
            "using regular AdamW\n",
            "step    1/10 | train loss 5.269999 | norm 30.4977 | lr 1.00e-04 | (8517.65 ms | 30 tok/s)\n",
            "step    2/10 | train loss 4.060640 | norm 17.0807 | lr 1.00e-04 | (5087.79 ms | 50 tok/s)\n",
            "step    3/10 | train loss 3.320009 | norm 14.7949 | lr 1.00e-04 | (5240.93 ms | 49 tok/s)\n",
            "step    4/10 | train loss 2.717534 | norm 13.2053 | lr 1.00e-04 | (4556.80 ms | 56 tok/s)\n",
            "step    5/10 | train loss 2.181174 | norm 12.4084 | lr 1.00e-04 | (4646.27 ms | 55 tok/s)\n",
            "step    6/10 | train loss 1.654125 | norm 10.6530 | lr 1.00e-04 | (5772.42 ms | 44 tok/s)\n",
            "step    7/10 | train loss 1.168180 | norm 9.7966 | lr 1.00e-04 | (4899.87 ms | 52 tok/s)\n",
            "step    8/10 | train loss 0.737264 | norm 8.1402 | lr 1.00e-04 | (5667.16 ms | 45 tok/s)\n",
            "step    9/10 | train loss 0.401494 | norm 6.2850 | lr 1.00e-04 | (4597.70 ms | 56 tok/s)\n",
            "step   10/10 | train loss 0.187752 | norm 3.6713 | lr 1.00e-04 | (4647.57 ms | 55 tok/s)\n",
            "final 9 iters avg: 5012.946ms\n",
            "peak memory consumption: 0 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load tokenizer:\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Generate text\n",
        "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\")\n",
        "generated_ids = model.generate(input_ids, max_new_tokens=100, temperature=1.0, top_k=50)\n",
        "\n",
        "# Decode & print output\n",
        "print(\"\\nGenerated Text:\\n\", tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVBB1y_Q5QZ1",
        "outputId": "2802a908-2dcf-4104-cc83-ce1c0750d43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            " Once upon a time they looked small; now they are awful: they have been cut off, they have been taken, they have come: they have paid us: they have given us: we\n",
            "know Them well, we \n",
            "\n",
            " \n",
            "\n",
            "5\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "First Citizen:\n",
            "\n",
            "We know you: you are few,\n",
            "are nearly all at once: that you are strangers to the people\n",
            "whom you are\n",
            "disguised, that you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"gpt2_tokenizer.bin\", \"rb\") as f:\n",
        "    text = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "    print(text[:5000])  # Show first 5000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3xK8mJARug",
        "outputId": "be9495c6-8770-4e15-dd9a-77c409ad4d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\u0001\u0002\u0000\u0000\u0000Q\u0000\u0000P\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001!\u0001\"\u0001#\u0001$\u0001%\u0001&\u0001'\u0001(\u0001)\u0001*\u0001+\u0001,\u0001-\u0001.\u0001/\u00010\u00011\u00012\u00013\u00014\u00015\u00016\u00017\u00018\u00019\u0001:\u0001;\u0001<\u0001=\u0001>\u0001?\u0001@\u0001A\u0001B\u0001C\u0001D\u0001E\u0001F\u0001G\u0001H\u0001I\u0001J\u0001K\u0001L\u0001M\u0001N\u0001O\u0001P\u0001Q\u0001R\u0001S\u0001T\u0001U\u0001V\u0001W\u0001X\u0001Y\u0001Z\u0001[\u0001\\\u0001]\u0001^\u0001_\u0001`\u0001a\u0001b\u0001c\u0001d\u0001e\u0001f\u0001g\u0001h\u0001i\u0001j\u0001k\u0001l\u0001m\u0001n\u0001o\u0001p\u0001q\u0001r\u0001s\u0001t\u0001u\u0001v\u0001w\u0001x\u0001y\u0001z\u0001{\u0001|\u0001}\u0001~\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0000\u0001\u0001\u0001\u0002\u0001\u0003\u0001\u0004\u0001\u0005\u0001\u0006\u0001\u0007\u0001\b\u0001\t\u0001\n",
            "\u0001\u000b\u0001\f\u0001\r\u0001\u000e\u0001\u000f\u0001\u0010\u0001\u0011\u0001\u0012\u0001\u0013\u0001\u0014\u0001\u0015\u0001\u0016\u0001\u0017\u0001\u0018\u0001\u0019\u0001\u001a\u0001\u001b\u0001\u001c\u0001\u001d\u0001\u001e\u0001\u001f\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0002 t\u0002 a\u0002he\u0002in\u0002re\u0002on\u0004 the\u0002er\u0002 s\u0002at\u0002 w\u0002 o\u0002en\u0002 c\u0002it\u0002is\u0002an\u0002or\u0002es\u0002 b\u0002ed\u0002 f\u0003ing\u0002 p\u0002ou\u0003 an\u0002al\u0002ar\u0003 to\u0002 m\u0003 of\u0003 in\u0002 d\u0002 h\u0004 and\u0002ic\u0002as\u0002le\u0003 th\u0003ion\u0002om\u0002ll\u0003ent\u0002 n\u0002 l\u0002st\u0003 re\u0002ve\u0002 e\u0002ro\u0002ly\u0003 be\u0002 g\u0002 T\u0002ct\u0002 S\u0002id\u0002ot\u0002 I\u0002ut\u0002et\u0002 A\u0003 is\u0003 on\u0002im\u0002am\u0002ow\u0002ay\u0002ad\u0002se\u0005 that\u0002 C\u0002ig\u0004 for\u0002ac\u0002 y\u0003ver\u0002ur\u0002 u\u0002ld\u0003 st\u0002 M\u0002's\u0003 he\u0003 it\u0005ation\u0003ith\u0002ir\u0002ce\u0004 you\u0002il\u0002 B\u0003 wh\u0002ol\u0002 P\u0005 with\u0002 1\u0003ter\u0002ch\u0003 as\u0003 we\u0002 (\u0002nd\u0003ill\u0002 D\u0002if\u0002 2\u0002ag\u0003ers\u0002ke\u0002 \"\u0002 H\u0002em\u0004 con\u0002 W\u0002 R\u0003her\u0004 was\u0002 r\u0002od\u0002 F\u0002ul\u0003ate\u0003 at\u0002ri\u0002pp\u0003ore\u0004 The\u0003 se\u0002us\u0004 pro\u0003 ha\u0002um\u0004 are\u0003 de\u0003ain\u0003and\u0003 or\u0003igh\u0003est\u0003ist\u0002ab\u0003rom\u0002 N\u0002th\u0004 com\u0002 G\u0002un\u0002op\u000200\u0002 L\u0004 not\u0003ess\u0003 ex\u0002 v\u0003res\u0002 E\u0002ew\u0003ity\u0003ant\u0003 by\u0002el\u0002os\u0003ort\u0002oc\u0002qu\u0005 from\u0005 have\u0003 su\u0003ive\u0004ould\u0003 sh\u0005 this\u0002nt\u0002ra\u0002pe\u0004ight\u0003art\u0004ment\u0003 al\u0003ust\u0003end\u0002--\u0003all\u0002 O\u0003ack\u0003 ch\u0003 le\u0003ies\u0003red\u0003ard\u0002\u0003out\u0002 J\u0003 ab\u0003ear\u0002iv\u0004ally\u0003our\u0003ost\u0002gh\u0002pt\u0003 pl\u0003ast\u0004 can\u0002ak\u0003ome\u0002ud\u0003The\u0004 his\u0003 do\u0003 go\u0004 has\u0002ge\u0002't\u0002 U\u0003rou\u0003 sa\u0002 j\u0004 but\u0004 wor\u0004 all\u0003ect\u0002 k\u0003ame\u0005 will\u0002ok\u0004 whe\u0005 they\u0003ide\u000201\u0002ff\u0003ich\u0002pl\u0004ther\u0003 tr\u0002..\u0004 int\u0002ie\u0003ure\u0003age\u0003 ne\u0003ial\u0002ap\u0003ine\u0003ice\u0003 me\u0004 out\u0003ans\u0003one\u0003ong\u0004ions\u0004 who\u0002 K\u0003 up\u0006 their\u0003 ad\u0002 3\u0003 us\u0004ated\u0003ous\u0005 more\u0002ue\u0002og\u0003 St\u0003ind\u0003ike\u0003 so\u0003ime\u0003per\u0002.\"\u0003ber\u0002iz\u0003act\u0004 one\u0005 said\u0002 -\u0003are\u0005 your\u0002cc\u0003 Th\u0003 cl\u0002ep\u0003ake\u0004able\u0002ip\u0005 cont\u0006 which\u0002ia\u0003 im\u0006 about\u0005 were\u0004very\u0002ub\u0004 had\u0003 en\u0005 comp\u0002,\"\u0003 In\u0003 un\u0003 ag\u0003ire\u0003ace\u0002au\u0003ary\u0006 would\u0003ass\u0002ry\u0003 \u0002cl\u0003ook\u0003ere\u0002so\u0002 V\u0003ign\u0002ib\u0004 off\u0003 te\u0003ven\u0002 Y\u0003ile\u0003ose\u0003ite\u0003orm\u0004 201\u0004 res\u0004 man\u0004 per\u0006 other\u0003ord\u0003ult\u0005 been\u0005 like\u0003ase\u0004ance\u0002ks\u0003ays\u0003own\u0004ence\u0004 dis\u0005ction\u0004 any\u0004 app\u0003 sp\u0003int\u0004ress\u0006ations\u0003ail\u0002 4\u0004ical\u0005 them\u0004 her\u0004ount\u0003 Ch\u0003 ar\u0003 if\u0006 there\u0003 pe\u0005 year\u0002av\u0003 my\u0005 some\u0005 when\u0004ough\u0003ach\u0005 than\u0002ru\u0003ond\u0003ick\u0005 over\u0003vel\u0003 qu\u0002\n",
            "\n",
            "\u0003 sc\u0004reat\u0003ree\u0003 It\u0004ound\u0004port\u0005 also\u0005 part\u0004fter\u0003 kn\u0004 bec\u0005 time\u0003ens\u0002 5\u0004ople\u0005 what\u0003 no\u0002du\u0003mer\u0003ang\u0004 new\u0004----\u0004 get\u0003ory\u0005ition\u0004ings\u0005 just\u0005 into\u0002 0\u0004ents\u0003ove\u0002te\u0007 people\u0004 pre\u0004 its\u0004 rec\u0003 tw\u0003ian\u0004irst\u0003ark\u0003ors\u0005 work\u0003ade\u0002ob\u0004 she\u0004 our\u0002wn\u0003ink\u0003lic\u0003 19\u0003 He\u0003ish\u0004nder\u0004ause\u0004 him\u0003ons\u0002 [\u0003 ro\u0004form\u0003ild\u0004ates\u0004vers\u0005 only\u0003oll\u0004 spe\u0002ck\u0003ell\u0003amp\u0004 acc\u0003 bl\u0004ious\u0003urn\u0002ft\u0003ood\u0004 how\u0003hed\u0002 '\u0006 after\u0002aw\u0004 att\u0002ov\u0002ne\u0005 play\u0003erv\u0003ict\u0006 could\u0003itt\u0003 am\u0006 first\u0002 6\u0004 act\u0002 $\u0002ec\u0004hing\u0003ual\u0003ull\u0005 comm\u0002oy\u0003old\u0003ces\u0004ater\u0003 fe\u0004 bet\u0002we\u0003iff\u0004 two\u0003ock\u0005 back\u0002).\u0005ident\u0006 under\u0005rough\u0003sel\u0002xt\u0004 may\u0005round\u0003 po\u0002ph\u0003iss\u0004 des\u0005 most\u0004 did\u0004 add\u0004ject\u0004 inc\u0004fore\u0004 pol\u0003ont\u0006 again\u0004clud\u0004tern\u0005 know\u0005 need\u0005 cons\u0003 co\u0002 .\u0005 want\u0004 see\u0002 7\u0004ning\u0003iew\u0005 This\u0003ced\u0005 even\u0004 ind\u0002ty\u0003 We\u0003ath\u0006 these\u0003 pr\u0004 use\b because\u0003 fl\u0002ng\u0004 now\u0004 \u0003com\u0003ise\u0005 make\u0005 then\u0004ower\u0006 every\u0003 Un\u0004 sec\u0003oss\u0003uch\u0003 em\u0002 =\u0003 Re\u0003ied\u0003rit\u0004 inv\u0004lect\u0005 supp\u0005ating\u0005 look\u0003man\u0004pect\u0002 8\u0003row\u0003 bu\u0006 where\u0004ific\u0006 years\u0003ily\u0005 diff\u0007 should\u0004 rem\u0002Th\u0002In\u0003 ev\u0003day\u0003're\u0003rib\u0004 rel\u0002ss\u0004 def\u0006 right\u0003 sy\u0002),\u0003les\u0003000\u0003hen\b through\u0003 Tr\u0002__\u0004 way\u0004 don\u0002 ,\u0003 10\u0004ased\u0004 ass\u0005ublic\u0004 reg\u0004 And\u0002ix\u0005 very\u0007 includ\u0005other\u0004 imp\u0003oth\u0004 sub\u0004 \u0006 being\u0003arg\u0003 Wh\u0002==\u0004ible\u0005 does\u0004ange\u0003ram\u0002 9\u0003ert\u0002ps\u0004ited\u0007ational\u0003 br\u0005 down\u0005 many\u0005aking\u0005 call\u0005uring\u0005ities\u0003 ph\u0003ics\u0003als\u0004 dec\u0005ative\u0004ener\u0007 before\u0005ility\u0005 well\u0005 much\u0005erson\u0006 those\u0005 such\u0003 ke\u0004 end\u0004 But\u0004ason\u0004ting\u0005 long\u0002ef\u0006 think\u0002ys\u0004 bel\u0003 sm\u0003its\u0002ax\u0004 own\u0005 prov\u0004 set\u0003ife\u0005ments\u0003ble\u0004ward\u0005 show\u0005 pres\u0002ms\u0004omet\u0003 ob\u0004 say\u0003 Sh\u0002ts\u0003ful\u0004 eff\u0003 gu\u0005 inst\u0003und\u0003ren\u0004cess\u0004 ent\u0004 You\u0005 good\u0006 start\u0004ince\u0005 made\u0002tt\u0004stem\u0004olog\u0002up\u0002 |\u0003ump\u0004 hel\u0004vern\u0004ular\u0005ually\u0003 ac\u0004 mon\u0005 last\u0004 200\u000210\u0005 stud\u0004ures\u0003 Ar\u0004self\u0003ars\u0005meric\u0003ues\u0002cy\u0004 min\u0005ollow\u0004 col\u0002io\u0004 mod\u0006 count\u0004 Com\u0003hes\u0004 fin\u0003air\u0003ier\u0003\u0004read\u0003ank\u0004atch\u0004ever\u0004 str\u0006 point\u0003ork\u0004 New\u0004 sur\u0003ool\u0003alk\u0005ement\u0005 used\u0004ract\u0004ween\u0005 same\u0003oun\u0003 Al\u0002ci\b differe\u0006 while\b--------\u0005 game\u0004cept\u0004 sim\u0003...\u0006 inter\u0002ek\u0007 report\u0006 produ\u0006 still\u0003led\u0002ah\u0005 here\u0006 world\u0007 though\u0004 num\u0004arch\u0004imes\u0003ale\u0003 Se\u0003 If\u0002//\u0003 Le\u0004 ret\u0004 ref\u0006 trans\u0003ner\u0005ution\u0004ters\u0005 take\u0003 Cl\u0005 conf\u0003way\u0003ave\u0006 going\u0003 sl\u0002ug\u0007 Americ\u0005 spec\u0005 hand\b between\u0004ists\u0003 De\u0003oot\u0002It\u0004 ear\b against\u0005 high\u0003gan\u0002az\u0005ather\u0004 exp\u0003 op\u0004 ins\u0003 gr\u0005 help\u0005 requ\u0003ets\u0003ins\u0004 Pro\u0003ism\u0006 found\u0004land\u0003ata\u0003uss\u0004ames\u0007 person\u0006 great\u0002pr\u0005 sign\u0003 An\u0003've\u0006 somet\u0004 ser\u0003hip\u0004 run\u0002 :\u0004 ter\u0005irect\u0007 follow\u0004 det\u0004ices\u0005 find\u000212\u0004 mem\u0003 cr\u0004ered\u0002ex\u0004 ext\u0003uth\u0004ense\u0002co\u0005 team\u0004v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "text = enc.encode(\"Hello World I am here ! Bye Bye .\")\n",
        "Tens = torch.tensor(text)\n",
        "print(Tens)\n",
        "x = Tens[:-1]\n",
        "y = Tens[1:]\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "fDTpzuXXeJca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120599d9-318c-4a19-cdca-7434a1e15b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([15496,  2159,   314,   716,   994,  5145, 47843, 47843,   764])\n",
            "tensor([15496,  2159,   314,   716,   994,  5145, 47843, 47843])\n",
            "tensor([ 2159,   314,   716,   994,  5145, 47843, 47843,   764])\n"
          ]
        }
      ]
    }
  ]
}
